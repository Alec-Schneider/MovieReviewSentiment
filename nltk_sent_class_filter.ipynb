{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d88352eb-f102-421c-808e-a001c5f85e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk.collocations as collocations\n",
    "from nltk.collocations import *\n",
    "import re\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6650d52a-d184-4195-87f4-0b182ae7e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./movie_reviews/train.tsv', 'r')\n",
    "# loop over lines in the file and use the first limit of them\n",
    "phrasedata = []\n",
    "for line in f:\n",
    "  # ignore the first line starting with Phrase and read all lines\n",
    "  if (not line.startswith('Phrase')):\n",
    "    # remove final end of line character\n",
    "    line = line.strip()\n",
    "    # each line has 4 items separated by tabs\n",
    "    # ignore the phrase and sentence ids, and keep the phrase and sentiment\n",
    "    phrasedata.append(line.split('\\t')[2:4])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a2ff611-363b-46ff-8784-3fefbd4fa289",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gfqhHHOlE_4z",
    "outputId": "62e3f55b-fcb4-47e6-97df-d53b5ffae254"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['too erotic', '1']\n",
      "['the illogic of its characters', '1']\n",
      "['in favour of an approach', '2']\n",
      "['quirky characters , odd situations , and off-kilter dialogue', '2']\n",
      "['manners and', '2']\n",
      "['Alfonso', '2']\n",
      "[\"with a contemptible imitator starring a `` SNL '' has-been acting like an 8-year-old channeling Roberto Benigni\", '0']\n",
      "[\"dismissed heroes would be a film that is n't this painfully forced , false and fabricated .\", '0']\n",
      "[\"be distracted by the movie 's quick movements\", '2']\n",
      "['a pointless , meandering celebration', '1']\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(phrasedata)\n",
    "phraselist = phrasedata\n",
    "for phrase in phraselist[:10]:\n",
    "    print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3d226e-2d2f-4265-9d25-6cdc5c0c0e72",
   "metadata": {
    "id": "8SA067zPFqsd"
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "phrasedocs = []\n",
    "for phrase in phraselist:\n",
    "    tokens = tokenizer.tokenize(phrase[0])\n",
    "    phrasedocs.append((tokens, int(phrase[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f0b96db-3422-44b6-89af-718526110f6c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['frank', 'humanity'], 2),\n",
       " (['most', 'slyly', 'exquisite', 'anti', 'adult', 'movies'], 3),\n",
       " (['tight', 'and', 'truthful'], 3),\n",
       " (['to', 'their', 'characters'], 2),\n",
       " (['Same',\n",
       "   'song',\n",
       "   'second',\n",
       "   'verse',\n",
       "   'coulda',\n",
       "   'been',\n",
       "   'better',\n",
       "   'but',\n",
       "   'it',\n",
       "   'coulda',\n",
       "   'been',\n",
       "   'worse'],\n",
       "  2),\n",
       " (['under',\n",
       "   'the',\n",
       "   'right',\n",
       "   'conditions',\n",
       "   'it',\n",
       "   's',\n",
       "   'goofy',\n",
       "   'LRB',\n",
       "   'if',\n",
       "   'not',\n",
       "   'entirely',\n",
       "   'wholesome',\n",
       "   'RRB',\n",
       "   'fun'],\n",
       "  3),\n",
       " (['reopens',\n",
       "   'an',\n",
       "   'interesting',\n",
       "   'controversy',\n",
       "   'and',\n",
       "   'never',\n",
       "   'succumbs',\n",
       "   'to',\n",
       "   'sensationalism'],\n",
       "  3),\n",
       " (['sell', 'many', 'records'], 3),\n",
       " (['Gives', 'an', 'intriguing', 'twist'], 3),\n",
       " (['for', 'artistic', 'integrity'], 2),\n",
       " (['of', 'our', 'time'], 2),\n",
       " (['demonstrated',\n",
       "   'a',\n",
       "   'knack',\n",
       "   'for',\n",
       "   'mixing',\n",
       "   'action',\n",
       "   'and',\n",
       "   'idiosyncratic',\n",
       "   'humor',\n",
       "   'in',\n",
       "   'his',\n",
       "   'charming',\n",
       "   '2000',\n",
       "   'debut',\n",
       "   'Shanghai',\n",
       "   'Noon'],\n",
       "  3),\n",
       " (['a', 'cinematic', 'year'], 2),\n",
       " (['Works', 'as', 'pretty', 'contagious', 'fun'], 3),\n",
       " (['the', 'level', 'of', 'insight', 'that', 'made', 'LRB', 'Eyre', 's', 'RRB'],\n",
       "  2),\n",
       " (['Jackass',\n",
       "   'is',\n",
       "   'a',\n",
       "   'vulgar',\n",
       "   'and',\n",
       "   'cheap',\n",
       "   'looking',\n",
       "   'version',\n",
       "   'of',\n",
       "   'Candid',\n",
       "   'Camera',\n",
       "   'staged',\n",
       "   'for',\n",
       "   'the',\n",
       "   'Marquis',\n",
       "   'de',\n",
       "   'Sade',\n",
       "   'set'],\n",
       "  0),\n",
       " (['can', 'be', 'said', 'of', 'the', 'picture'], 2),\n",
       " (['Bicentennial', 'Man'], 2),\n",
       " (['a', 'gut', 'wrenching', 'examination'], 2),\n",
       " (['its',\n",
       "   'exquisite',\n",
       "   'acting',\n",
       "   'inventive',\n",
       "   'screenplay',\n",
       "   'mesmerizing',\n",
       "   'music',\n",
       "   'and'],\n",
       "  4),\n",
       " (['Self',\n",
       "   'congratulatory',\n",
       "   'misguided',\n",
       "   'and',\n",
       "   'ill',\n",
       "   'informed',\n",
       "   'if',\n",
       "   'nonetheless',\n",
       "   'compulsively',\n",
       "   'watchable'],\n",
       "  2),\n",
       " (['disapproval',\n",
       "   'of',\n",
       "   'Justine',\n",
       "   'combined',\n",
       "   'with',\n",
       "   'a',\n",
       "   'tinge',\n",
       "   'of',\n",
       "   'understanding',\n",
       "   'for',\n",
       "   'her',\n",
       "   'actions'],\n",
       "  2),\n",
       " (['the', 'vehicle', 'for', 'Chan'], 2),\n",
       " (['cut', 'out'], 2),\n",
       " (['exasperating'], 1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrasedocs[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c20b550e-585c-4f1b-9133-25e66f9bfea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "no_stop_phrases = []\n",
    "for phrase, label in phrasedocs:\n",
    "    phrase = [word for word in phrase if word not in stop_words]\n",
    "    no_stop_phrases.append((phrase, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9d23cc8-0c1d-43f7-a9e5-094f5f59b65e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['frank', 'humanity'], 2),\n",
       " (['slyly', 'exquisite', 'anti', 'adult', 'movies'], 3),\n",
       " (['tight', 'truthful'], 3),\n",
       " (['characters'], 2),\n",
       " (['Same', 'song', 'second', 'verse', 'coulda', 'better', 'coulda', 'worse'],\n",
       "  2),\n",
       " (['right',\n",
       "   'conditions',\n",
       "   'goofy',\n",
       "   'LRB',\n",
       "   'entirely',\n",
       "   'wholesome',\n",
       "   'RRB',\n",
       "   'fun'],\n",
       "  3),\n",
       " (['reopens',\n",
       "   'interesting',\n",
       "   'controversy',\n",
       "   'never',\n",
       "   'succumbs',\n",
       "   'sensationalism'],\n",
       "  3),\n",
       " (['sell', 'many', 'records'], 3),\n",
       " (['Gives', 'intriguing', 'twist'], 3),\n",
       " (['artistic', 'integrity'], 2),\n",
       " (['time'], 2),\n",
       " (['demonstrated',\n",
       "   'knack',\n",
       "   'mixing',\n",
       "   'action',\n",
       "   'idiosyncratic',\n",
       "   'humor',\n",
       "   'charming',\n",
       "   '2000',\n",
       "   'debut',\n",
       "   'Shanghai',\n",
       "   'Noon'],\n",
       "  3),\n",
       " (['cinematic', 'year'], 2),\n",
       " (['Works', 'pretty', 'contagious', 'fun'], 3),\n",
       " (['level', 'insight', 'made', 'LRB', 'Eyre', 'RRB'], 2),\n",
       " (['Jackass',\n",
       "   'vulgar',\n",
       "   'cheap',\n",
       "   'looking',\n",
       "   'version',\n",
       "   'Candid',\n",
       "   'Camera',\n",
       "   'staged',\n",
       "   'Marquis',\n",
       "   'de',\n",
       "   'Sade',\n",
       "   'set'],\n",
       "  0),\n",
       " (['said', 'picture'], 2),\n",
       " (['Bicentennial', 'Man'], 2),\n",
       " (['gut', 'wrenching', 'examination'], 2),\n",
       " (['exquisite', 'acting', 'inventive', 'screenplay', 'mesmerizing', 'music'],\n",
       "  4),\n",
       " (['Self',\n",
       "   'congratulatory',\n",
       "   'misguided',\n",
       "   'ill',\n",
       "   'informed',\n",
       "   'nonetheless',\n",
       "   'compulsively',\n",
       "   'watchable'],\n",
       "  2),\n",
       " (['disapproval', 'Justine', 'combined', 'tinge', 'understanding', 'actions'],\n",
       "  2),\n",
       " (['vehicle', 'Chan'], 2),\n",
       " (['cut'], 2),\n",
       " (['exasperating'], 1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_stop_phrases[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a497fa6-e49c-4545-a117-5c18d915a0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['erotic'], 1)\n",
      "(['illogic', 'characters'], 1)\n",
      "(['favour', 'approach'], 2)\n",
      "(['quirky', 'characters', 'odd', 'situations', 'kilter', 'dialogue'], 2)\n",
      "(['manners'], 2)\n",
      "(['alfonso'], 2)\n",
      "(['contemptible', 'imitator', 'starring', 'snl', 'acting', 'like', '8', 'year', 'old', 'channeling', 'roberto', 'benigni'], 0)\n",
      "(['dismissed', 'heroes', 'would', 'film', 'n', 'painfully', 'forced', 'false', 'fabricated'], 0)\n",
      "(['distracted', 'movie', 'quick', 'movements'], 2)\n",
      "(['pointless', 'meandering', 'celebration'], 1)\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "for phrase in no_stop_phrases:\n",
    "    lowerphrase = ([w.lower() for w in phrase[0]], phrase[1])\n",
    "    docs.append(lowerphrase)\n",
    "for phrase in docs[:10]:\n",
    "    print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fa2a139-ac4d-4e61-af96-07cbfac16d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15253\n"
     ]
    }
   ],
   "source": [
    "all_words_list = [word for (sent, cat) in docs for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90371ebd-7fdc-4513-8f56-8913a768c9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_items = all_words.most_common(1000)\n",
    "word_features = [word for (word, count) in word_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5339a3d8-5fba-4521-a35a-84130077cb37",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 6733),\n",
       " ('movie', 6241),\n",
       " ('the', 4668),\n",
       " ('n', 4025),\n",
       " ('one', 3784),\n",
       " ('like', 3190),\n",
       " ('a', 2780),\n",
       " ('story', 2539),\n",
       " ('rrb', 2438),\n",
       " ('good', 2261)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_items[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "259f5ae3-16c9-472c-83ac-5e2bbf4b82cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['film', 'movie', 'the', 'n', 'one', 'like', 'a', 'story', 'rrb', 'good']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_features[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7520f20-b43f-4cba-9054-5b470b7b5e89",
   "metadata": {},
   "source": [
    "## Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ae61573-0c49-48c7-a718-9e755537c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    #we open a Pytnon dictionary instead of a list\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        #checking if the word from word_features matches a word in the document\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eaea4707-4752-4566-ab35-7b4be897d16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alec\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "doc_chunks = np.array_split(np.array(docs), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf0fbd16-ad34-4705-b27b-05c940e46248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "feature_set_chunks = []\n",
    "for chunk in doc_chunks:\n",
    "    feature_set_chunks.append(\n",
    "        [(document_features(chunk_sent[0], word_features), chunk_sent[1]) \n",
    "         for chunk_sent in chunk]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d610acdc-7bad-4936-ad1e-c6dcec862bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = list(itertools.chain.from_iterable(feature_set_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "83d5800d-ae19-47ef-9e00-dcd756112d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'all_words_list' is not defined\n",
      "name 'phrasedata' is not defined\n",
      "name 'phrasedocs' is not defined\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del doc_chunks\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    del feature_set_chunks\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    del all_words_list\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    del all_words_list\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    del phrasedata\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    del phrasedocs\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "169d93e8-c851-4bc1-8fb7-8628b7207d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del no_stop_phrases\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2acaa45-1b22-4372-9ed1-b97171dfd635",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 0.0003\n",
      "Out 0.0002\n",
      "get_ipython 0.0001\n",
      "exit 0.0001\n",
      "quit 0.0001\n",
      "pd 0.0001\n",
      "nltk 0.0001\n",
      "plt 0.0001\n",
      "sns 0.0001\n",
      "np 0.0001\n",
      "collocations 0.0001\n",
      "BigramCollocationFinder 0.0011\n",
      "TrigramCollocationFinder 0.0011\n",
      "QuadgramCollocationFinder 0.0011\n",
      "re 0.0001\n",
      "KFold 0.0011\n",
      "precision_score 0.0001\n",
      "recall_score 0.0001\n",
      "f1_score 0.0001\n",
      "confusion_matrix 0.0001\n",
      "pickle 0.0001\n",
      "stopwords 0.0017\n",
      "random 0.0001\n",
      "itertools 0.0001\n",
      "f 0.0002\n",
      "line 0.0001\n",
      "phraselist 1.3208\n",
      "phrase 0.0001\n",
      "tokenizer 0.0001\n",
      "tokens 0.0001\n",
      "stop_words 0.0084\n",
      "no_stop_phrases 1.3208\n",
      "label 0.0\n",
      "docs 1.3208\n",
      "lowerphrase 0.0001\n",
      "all_words 0.5899\n",
      "word_items 0.009\n",
      "word_features 0.009\n",
      "alpha 0.0001\n",
      "bigram_measures 0.0001\n",
      "finder 0.0001\n",
      "scored 0.1185\n",
      "bigram_features 0.0043\n",
      "bi_document_features 0.0001\n",
      "chunk 0.0001\n",
      "featuresets2 1.2529\n",
      "sys 0.0001\n",
      "var_mems 0.0001\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "var_mems = []\n",
    "local_vars = list(locals().items())\n",
    "for var, obj in local_vars:\n",
    "    if not var.startswith(\"_\"):\n",
    "        print(var, np.round(sys.getsizeof(obj) / 1e6, 4))\n",
    "#         var_mems.append(var, np.round(sys.getsizeof(obj) / 1e6, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe525200-768e-4bd6-902a-cf53ec9a2627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def kfold_naive_bayes(feature_sets, splits=5):\n",
    "    kf = KFold(n_splits=splits)\n",
    "    scores = []\n",
    "    start_time = time.time()\n",
    "    for i, (train, test) in enumerate(kf.split(feature_sets)):\n",
    "        print(f\"split {i+1} being trained\")\n",
    "        classifier = nltk.NaiveBayesClassifier.train(\n",
    "            np.array(feature_sets)[train]\n",
    "        )\n",
    "        scores.append(\n",
    "            nltk.classify.accuracy(classifier, \n",
    "                                   np.array(feature_sets)[test])\n",
    "        )\n",
    "    \n",
    "        print(f\"Split {i+1} training time: {(time.time() - start_time) / 60:5.2f}mins\")\n",
    "    return classifier, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82aa0156-eb74-4c40-942a-7b5262fcf548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(true_labels, predictions):\n",
    "    precision = precision_score(true_labels, predictions, average='macro').round(4)\n",
    "    recall = recall_score(true_labels, predictions, average='macro').round(4)\n",
    "    f_measure = f1_score(true_labels, predictions, average='macro').round(4)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F Measure: \", f_measure)\n",
    "    return precision, recall, f_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "401bfd37-4c10-4113-9192-0c446a51538e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split 0 being trained\n",
      "split 0 being trained\n",
      "split 0 being trained\n",
      "split 0 being trained\n",
      "split 0 being trained\n",
      "split 0 being trained\n",
      "split 0 being trained\n",
      "split 0 being trained\n",
      "split 0 being trained\n",
      "split 0 being trained\n",
      "split 0 being trained\n",
      "split 0 being trained\n",
      "split 0 being trained\n",
      "split 0 being trained\n",
      "split 0 being trained\n",
      "Split 0 training time: 87.74mins\n",
      "Wall time: 1h 27min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "uni_15fold_1000words_results = kfold_naive_bayes(featuresets, splits=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c3d299e3-fc4f-4bf7-9e42-c311d733fe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./models/uni_1000word_model_15fold.pickle\", 'wb') as f:\n",
    "    pickle.dump(uni_15fold_1000words_results[0], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6c3f5067-f1cb-4b03-a5bb-1d396beae5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold Accuracies:\n",
      " [0.5725682429834679, 0.5705497885428681, 0.565359477124183, 0.5685313341022683, 0.5676662821991542, 0.5695886197616301, 0.5689158016147635, 0.5789119569396386, 0.5799692425990004, 0.5717031910803537, 0.5705497885428681, 0.5665128796616686, 0.5779507881584006, 0.5771818531334102, 0.5681468665897732] \n",
      "\n",
      "Mean Accuracy: 0.572\n"
     ]
    }
   ],
   "source": [
    "print(f\"Fold Accuracies:\\n {uni_15fold_1000words_results[1]} \\n\")\n",
    "print(f\"Mean Accuracy: {np.mean(uni_15fold_1000words_results[1]).round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "09389bf0-f1b7-45dc-a396-7c1cad3a321c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "uni_15fold_preds_labs = [(uni_15fold_1000words_results[0].classify(features), label) for features, label in featuresets] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "09506126-552c-488b-b1bf-d147ee247849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.488\n",
      "Recall:  0.3806\n",
      "F Measure:  0.4084\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "trues = []\n",
    "for pred, true in uni_15fold_preds_labs:\n",
    "    preds.append(pred)\n",
    "    trues.append(true)\n",
    "\n",
    "trues_preds_15fold = [trues, preds]\n",
    "uni_15fold_scores = score_model(trues_preds_15fold[0], trues_preds_15fold[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad6d2d7-ba78-4edd-85f1-a0d4d219c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(zip(trues_preds_15fold), columns="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7890dc9c-ec35-4ab8-a6c5-ef89ea8ea797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1544,  1899,  3094,   446,    89],\n",
       "       [ 1219,  6898, 16703,  2132,   321],\n",
       "       [  643,  4405, 69273,  4753,   508],\n",
       "       [  322,  1932, 18703, 10377,  1593],\n",
       "       [   81,   403,  3030,  3425,  2267]], dtype=int64)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(trues_preds_15fold[0], trues_preds_15fold[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7de340d-f68d-4839-a78f-28f05b2ca209",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e82c330-6cda-47b3-ab50-a9069d798d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('10th', 'angry'), 6.556087327083197e-05),\n",
       " (('12th', 'choosing'), 6.556087327083197e-05),\n",
       " (('13th', 'incarnation'), 6.556087327083197e-05),\n",
       " (('15th', 'ease'), 6.556087327083197e-05),\n",
       " (('1930s', 'nightmares'), 6.556087327083197e-05),\n",
       " (('1940s', 'aspirations'), 6.556087327083197e-05),\n",
       " (('1950s', 'ww'), 6.556087327083197e-05),\n",
       " (('1960s', 'spaces'), 6.556087327083197e-05),\n",
       " (('1970s', 'macaroni'), 6.556087327083197e-05),\n",
       " (('1980s', 'ascension'), 6.556087327083197e-05)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def alpha(w):\n",
    "    pattern = re.compile('^[^a-z]+$')\n",
    "    if(pattern.match(w)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#creating bigrams features for the corpus and applying cleaning steps\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(all_words)\n",
    "finder.apply_word_filter(alpha)\n",
    "finder.apply_word_filter(lambda w: w in stopwords)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "scored[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edbd39c3-37b4-410f-b643-8bac855d4b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('10th', 'angry'),\n",
       " ('12th', 'choosing'),\n",
       " ('13th', 'incarnation'),\n",
       " ('15th', 'ease'),\n",
       " ('1930s', 'nightmares'),\n",
       " ('1940s', 'aspirations'),\n",
       " ('1950s', 'ww'),\n",
       " ('1960s', 'spaces'),\n",
       " ('1970s', 'macaroni'),\n",
       " ('1980s', 'ascension'),\n",
       " ('19th', 'century'),\n",
       " ('20th', 'footnotes'),\n",
       " ('21st', 'latino'),\n",
       " ('30s', '40s'),\n",
       " ('3d', 'ungainly'),\n",
       " ('40s', 'ultra'),\n",
       " ('4ever', 'indecent'),\n",
       " ('4th', 'fun'),\n",
       " ('4w', 'resolutions'),\n",
       " ('50s', 'wyman'),\n",
       " ('51st', 'fulford'),\n",
       " ('5ths', 'xerox'),\n",
       " ('60s', 'sword'),\n",
       " ('65th', 'reunion'),\n",
       " ('70s', 'perhaps'),\n",
       " ('7th', 'oral'),\n",
       " ('80s', 'dreamscape'),\n",
       " ('8th', 'drastic'),\n",
       " ('90s', 'typifies'),\n",
       " ('aaa', 'lip')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracting clean bigrams (no frequency information)\n",
    "bigram_features = [bigram for (bigram, count) in scored[:500]]\n",
    "#printing the first 30 for confirmation\n",
    "bigram_features[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03daaef6-bc24-4e11-bc7f-905f57e61c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_document_features(document, bigram_features):\n",
    "    document_words = list(nltk.bigrams(document))\n",
    "    features = {}\n",
    "    for word in bigram_features:\n",
    "        #boolean logic will retunt 'True' if there is a match, or 'False' if not\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5e463a3-43b3-4b5b-8fe8-df99be7e1d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alec\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "doc_chunks = np.array_split(np.array(docs), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9f7af6e-825a-4c02-8532-ce1e6a64d704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "feature_set_chunks = []\n",
    "for chunk in doc_chunks:\n",
    "    feature_set_chunks.append(\n",
    "        [(bi_document_features(chunk_sent[0], bigram_features), chunk_sent[1]) \n",
    "         for chunk_sent in chunk]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4e45f5e-05d5-4117-8051-3bc44cf92c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets2 = list(itertools.chain.from_iterable(feature_set_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d900b14-f000-49f9-b462-7220a1829237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'all_words_list' is not defined\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del doc_chunks\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    del feature_set_chunks\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    del all_words_list\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    del all_words_list\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    del phrasedata\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    del phrasedocs\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    del \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    del \n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "199b2831-1b69-4d13-b99e-3afc715e4f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0be38a67-837b-4efb-b9ce-63725c6e26ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split 1 being trained\n",
      "Split 1 training time:  3.62mins\n",
      "split 2 being trained\n",
      "Split 2 training time:  7.15mins\n",
      "split 3 being trained\n",
      "Split 3 training time: 10.93mins\n",
      "split 4 being trained\n",
      "Split 4 training time: 13.91mins\n",
      "split 5 being trained\n",
      "Split 5 training time: 16.72mins\n",
      "Wall time: 16min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bi_5fold_results = kfold_naive_bayes(featuresets2, splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6eaf30aa-dda6-4ed3-a085-4ed678fc838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./models/bi_5fold_results_model.pickle\", 'wb') as f:\n",
    "    pickle.dump(bi_5fold_results[0], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e4c731e-e953-4228-925a-fa607f287f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold Accuracies:\n",
      " [0.5069204152249135, 0.5142573369216967, 0.5124311162373446, 0.5073369216967832, 0.5116942201717288] \n",
      "\n",
      "Mean Accuracy: 0.511\n"
     ]
    }
   ],
   "source": [
    "print(f\"Fold Accuracies:\\n {bi_5fold_results[1]} \\n\")\n",
    "print(f\"Mean Accuracy: {np.mean(bi_5fold_results[1]).round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d219207-6f7e-4045-b36e-85c1a1874b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bi_5fold_preds_labs = [(bi_5fold_results[0].classify(features), label) for features, label in featuresets2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bf55e54-5548-4244-be20-2f883ba42d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.6066\n",
      "Recall:  0.2027\n",
      "F Measure:  0.1409\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "trues = []\n",
    "for pred, true in bi_5fold_preds_labs:\n",
    "    preds.append(pred)\n",
    "    trues.append(true)\n",
    "\n",
    "trues_preds_5fold_bi = [trues, preds]\n",
    "bi_5fold_scores = score_model(trues_preds_5fold_bi[0], trues_preds_5fold_bi[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba7961b-6434-4f0d-8cb3-38e44a8e8164",
   "metadata": {},
   "source": [
    "## Part-Of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6398e4e-4f09-4255-a42e-e53eb66d8511",
   "metadata": {
    "id": "iHiJDvNBJEz9"
   },
   "outputs": [],
   "source": [
    "# this function takes a document list of words and returns a feature dictionary\n",
    "# it runs the default pos tagger (the Stanford tagger) on the document\n",
    "#   and counts 4 types of pos tags to use as features\n",
    "def POS_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    tagged_words = nltk.pos_tag(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    numNoun = 0\n",
    "    numVerb = 0\n",
    "    numAdj = 0\n",
    "    numAdverb = 0\n",
    "    for (word, tag) in tagged_words:\n",
    "        if tag.startswith('N'): numNoun += 1\n",
    "        if tag.startswith('V'): numVerb += 1\n",
    "        if tag.startswith('J'): numAdj += 1\n",
    "        if tag.startswith('R'): numAdverb += 1\n",
    "    features['nouns'] = numNoun\n",
    "    features['verbs'] = numVerb\n",
    "    features['adjectives'] = numAdj\n",
    "    features['adverbs'] = numAdverb\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b62800a7-c6f6-41a8-9d34-587157420dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alec\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "doc_chunks = np.array_split(np.array(docs), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6dd41de-730b-45a3-9db3-2e7c0f2c1da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "feature_set_chunks = []\n",
    "for chunk in doc_chunks:\n",
    "    feature_set_chunks.append(\n",
    "        [(POS_features(chunk_sent[0], word_features), chunk_sent[1]) \n",
    "         for chunk_sent in chunk]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40283d25-3c57-4578-b02f-ee695ff332a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1004\n"
     ]
    }
   ],
   "source": [
    "POS_featuresets = list(itertools.chain.from_iterable(feature_set_chunks))\n",
    "print(len(POS_featuresets[0][0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06909bf1-4bf1-4ed5-8125-4941e59e0b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'all_words_list' is not defined\n",
      "name 'all_words_list' is not defined\n",
      "name 'phrasedata' is not defined\n",
      "name 'phrasedocs' is not defined\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    del doc_chunks\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    del feature_set_chunks\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    del all_words_list\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    del all_words_list\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    del phrasedata\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    del phrasedocs\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    del phraselist\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    del featuresets2\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    del no_stop_phrases\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f76614e-7ac3-4241-89fe-96736ba8e256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split 1 being trained\n",
      "Split 1 training time:  6.84mins\n",
      "split 2 being trained\n",
      "Split 2 training time: 13.14mins\n",
      "split 3 being trained\n",
      "Split 3 training time: 19.65mins\n",
      "split 4 being trained\n",
      "Split 4 training time: 26.39mins\n",
      "split 5 being trained\n",
      "Split 5 training time: 33.62mins\n",
      "Wall time: 33min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pos_5fold_results = kfold_naive_bayes(POS_featuresets, splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8987e245-787a-47b8-9022-671e3cd65f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./models/pos_5fold_results_model.pickle\", 'wb') as f:\n",
    "    pickle.dump(pos_5fold_results[0], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0e07d19-8ec6-433b-9b3e-88b6c9856c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold Accuracies:\n",
      " [0.5564206074586697, 0.5543380750993208, 0.5580545943867743, 0.5543380750993208, 0.5544021530180699] \n",
      "\n",
      "Mean Accuracy: 0.556\n"
     ]
    }
   ],
   "source": [
    "print(f\"Fold Accuracies:\\n {pos_5fold_results[1]} \\n\")\n",
    "print(f\"Mean Accuracy: {np.mean(pos_5fold_results[1]).round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e2d6316-0f0c-4a7b-9a02-295d872619a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pos_5fold_preds_labs = [(pos_5fold_results[0].classify(features), label) for features, label in POS_featuresets] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb8c7056-b396-4a29-8225-bd5d8355c601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.4382\n",
      "Recall:  0.3948\n",
      "F Measure:  0.4062\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "trues = []\n",
    "for pred, true in pos_5fold_preds_labs:\n",
    "    preds.append(pred)\n",
    "    trues.append(true)\n",
    "\n",
    "trues_preds_5fold_pos = [trues, preds]\n",
    "pos_5fold_scores = score_model(trues_preds_5fold_pos[0], trues_preds_5fold_pos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a417aec-7f18-4d1e-ad41-b688ae6cdd11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
